{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ILOwNBs9C4vM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "zmFx4Y7WDGvG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hOXOKtDNCzjA"
      },
      "outputs": [],
      "source": [
        "# Setting up LoRA class which adds two trainable matrices on top of the layers's original weights W\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, features_in, features_out, rank=1, alpha=1, device=device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mat_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
        "        self.mat_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
        "        nn.init.normal_(self.mat_A, mean=0, std=1)\n",
        "\n",
        "        self.scale = alpha / rank\n",
        "\n",
        "\n",
        "    def forward(self, W):\n",
        "        return W + torch.matmul(self.mat_B, self.mat_A).view(W.shape) * self.scale\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This function takes the layer as the input and sets the features_in,features_out\n",
        "#equal to the shape of the weight matrix. This will help the LoRA class to\n",
        "#initialize the A and B Matrices\n",
        "\n",
        "def layer_parametrization(layer, device, rank = 1, lora_alpha = 1):\n",
        "  features_in, features_out = layer.weight.shape\n",
        "  return LoRA(features_in, features_out, rank = rank, alpha = lora_alpha, device = device)"
      ],
      "metadata": {
        "id": "uFKImAJ9C5t7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "NY7CBtpIDSFE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a transform pipeline so that we can make the data training ready\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1301,),(0.3081,))])\n",
        "\n",
        "mnist_train = datasets.MNIST(root = './data', train = True, download = True, transform = transform)\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size = 10, shuffle = True)\n",
        "\n",
        "mnist_test = datasets.MNIST(root = './data', train = False, download = True, transform = transform)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size = 10, shuffle = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va_PbXC0DYWB",
        "outputId": "bbff2f47-490c-4e27-ec7b-c65a7c4d4a51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 105164975.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 40418983.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 29321273.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6088376.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a demanding model so that it has more parameters\n",
        "#1000 neurons for first layer and 2000 layers for the second\n",
        "\n",
        "class exp_clf(nn.Module):\n",
        "\n",
        "  def __init__(self, neurons_1 = 1000, neurons_2 = 2000):\n",
        "    super(exp_clf, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, neurons_1)\n",
        "    self.linear2 = nn.Linear(neurons_1, neurons_2)\n",
        "    self.linear3 = nn.Linear(neurons_2, 10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, img):\n",
        "    x = img.view(-1, 28*28)\n",
        "    x = self.relu(self.linear1(x))\n",
        "    x = self.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    return x\n",
        "\n",
        "exp = exp_clf().to(device)"
      ],
      "metadata": {
        "id": "7mYEJeD-D5b8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's create a training loop\n",
        "\n",
        "def train(train_loader, mod, epochs = 5, total_iterations_limits = None):\n",
        "  cel = nn.CrossEntropyLoss()\n",
        "  optim = torch.optim.Adam(mod.parameters(), lr=0.001)\n",
        "\n",
        "  total_iterations = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    mod.train()\n",
        "\n",
        "    loss_sum = 0\n",
        "    num_iteration = 0\n",
        "\n",
        "    data_iterator = tqdm(train_loader, desc = f'Number of Epoch:{epoch+1}')\n",
        "    if total_iterations_limits is not None:\n",
        "      data_iterator.total = total_iterations_limits\n",
        "    for data in data_iterator:\n",
        "      num_iteration += 1\n",
        "      total_iterations += 1\n",
        "      x, y = data\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      optim.zero_grad()\n",
        "      output = mod(x.view(-1, 28*28))\n",
        "      loss = cel(output, y)\n",
        "      loss_sum += loss.item()\n",
        "      avg_loss = loss_sum / num_iteration\n",
        "      data_iterator.set_postfix(loss = avg_loss)\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "\n",
        "      if total_iterations_limits is not None and total_iterations >= total_iterations_limits:\n",
        "        return"
      ],
      "metadata": {
        "id": "go5gYD4RF1GV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_loader, exp, epochs = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUwMaVgF--i",
        "outputId": "01c79928-ec5a-4236-bfc9-5897e8627168"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Number of Epoch:1: 100%|██████████| 6000/6000 [00:46<00:00, 129.25it/s, loss=0.237]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_weights = {}\n",
        "for name, params in exp.named_parameters():\n",
        "  original_weights[name] = params.clone().detach()"
      ],
      "metadata": {
        "id": "q6lZEuEzGqjm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the model and looking out for the wrongly identified cases\n",
        "def test():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  wrong_counts = [0 for i in range(10)]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in tqdm(test_loader, desc = 'testing'):\n",
        "      x, y = data\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      output = exp(x.view(-1, 784))\n",
        "      for idx, i in enumerate(output):\n",
        "        if torch.argmax(i) == y[idx]:\n",
        "          correct += 1\n",
        "        else:\n",
        "          wrong_counts[y[idx]] += 1\n",
        "        total +=1\n",
        "\n",
        "  print(f'\\nAccuracy: {round(correct/total, 3)}')\n",
        "  for i in range(len(wrong_counts)):\n",
        "    print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubnj55s3G6sg",
        "outputId": "d129314a-105d-4e26-c5d2-1847ef9cedcb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1000/1000 [00:03<00:00, 256.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.953\n",
            "wrong counts for the digit 0: 49\n",
            "wrong counts for the digit 1: 5\n",
            "wrong counts for the digit 2: 43\n",
            "wrong counts for the digit 3: 29\n",
            "wrong counts for the digit 4: 42\n",
            "wrong counts for the digit 5: 42\n",
            "wrong counts for the digit 6: 96\n",
            "wrong counts for the digit 7: 67\n",
            "wrong counts for the digit 8: 50\n",
            "wrong counts for the digit 9: 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = 0\n",
        "\n",
        "for idx, layer in enumerate([exp.linear1, exp.linear2, exp.linear3]):\n",
        "  total_params += layer.weight.nelement() + layer.bias.nelement()\n",
        "  print(f'Layer {idx+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
        "\n",
        "print('The total trainable parameters of our model are:', total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBjHfuulG8uL",
        "outputId": "baa1bb3b-ef17-4215-9bb7-9cbc14810be8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
            "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
            "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n",
            "The total trainable parameters of our model are: 2807010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Parametrizing such as when the model calls for weights, it gets weights plus the two trainable matrices introduced.\n",
        "\n",
        "import torch.nn.utils.parametrize as parametrize\n",
        "\n",
        "parametrize.register_parametrization(exp.linear1, 'weight', layer_parametrization(exp.linear1, device))\n",
        "parametrize.register_parametrization(exp.linear2, 'weight', layer_parametrization(exp.linear2, device))\n",
        "parametrize.register_parametrization(exp.linear3, 'weight', layer_parametrization(exp.linear3, device))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZinQkdC_G_ZQ",
        "outputId": "5c9c3b90-03b1-45ca-d15a-26840e733049"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParametrizedLinear(\n",
              "  in_features=2000, out_features=10, bias=True\n",
              "  (parametrizations): ModuleDict(\n",
              "    (weight): ParametrizationList(\n",
              "      (0): LoRA()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_parameters_lora = 0\n",
        "total_parameters_non_lora = 0\n",
        "\n",
        "for index, layer in enumerate([exp.linear1, exp.linear2, exp.linear3]):\n",
        "  total_parameters_lora += layer.parametrizations['weight'][0].mat_A.nelement() + layer.parametrizations['weight'][0].mat_B.nelement()\n",
        "  total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
        "  print(\n",
        "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + mat_A: {layer.parametrizations[\"weight\"][0].mat_A.shape} + mat_B: {layer.parametrizations[\"weight\"][0].mat_B.shape}'\n",
        "    )\n",
        "\n",
        "assert total_parameters_non_lora == total_params\n",
        "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
        "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
        "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
        "parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
        "print(f'Parameters incremment: {parameters_incremment:.3f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ESq2u6ZHI2S",
        "outputId": "66f1c4dc-e0d3-4b2d-f282-ca7b9bfe9480"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + mat_A: torch.Size([1, 784]) + mat_B: torch.Size([1000, 1])\n",
            "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + mat_A: torch.Size([1, 1000]) + mat_B: torch.Size([2000, 1])\n",
            "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + mat_A: torch.Size([1, 2000]) + mat_B: torch.Size([10, 1])\n",
            "Total number of parameters (original): 2,807,010\n",
            "Total number of parameters (original + LoRA): 2,813,804\n",
            "Parameters introduced by LoRA: 6,794\n",
            "Parameters incremment: 0.242%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in exp.named_parameters():\n",
        "    if 'mat' not in name:\n",
        "        print(f'Freezing non-LoRA parameter {name}')\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Load the MNIST dataset again, by keeping only the digit 7\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "exclude_indices = mnist_trainset.targets == 7\n",
        "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
        "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
        "# Create a dataloader for the training\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
        "\n",
        "for layer in [exp.linear1, exp.linear2, exp.linear3]:\n",
        "  layer.parametrizations[\"weight\"][0].requires_grad = True\n",
        "\n",
        "# Train the network with LoRA only on the digit 7 and only for 100 batches (hoping that it would improve the performance on the digit 7)\n",
        "train(train_loader, exp, epochs=1, total_iterations_limits=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHvsAps6JRsk",
        "outputId": "2e45ea3a-1250-45de-a696-ad1981c62971"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing non-LoRA parameter linear1.bias\n",
            "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
            "Freezing non-LoRA parameter linear2.bias\n",
            "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
            "Freezing non-LoRA parameter linear3.bias\n",
            "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Number of Epoch:1:  99%|█████████▉| 99/100 [00:00<00:00, 124.36it/s, loss=0.0307]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBzqalKiJ1Ro",
        "outputId": "5a6c267e-9479-477e-e07f-5768b3679aa8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1000/1000 [00:03<00:00, 280.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.893\n",
            "wrong counts for the digit 0: 52\n",
            "wrong counts for the digit 1: 247\n",
            "wrong counts for the digit 2: 144\n",
            "wrong counts for the digit 3: 55\n",
            "wrong counts for the digit 4: 66\n",
            "wrong counts for the digit 5: 55\n",
            "wrong counts for the digit 6: 111\n",
            "wrong counts for the digit 7: 1\n",
            "wrong counts for the digit 8: 137\n",
            "wrong counts for the digit 9: 205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The wrong counts for 7 has reduced to a great extent"
      ],
      "metadata": {
        "id": "XkggFTZzRuLh"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}